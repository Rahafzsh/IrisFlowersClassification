{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahafzsh/DL-Assignment/blob/main/DL_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ASSIGNMENT 1: Iris Data Classification (Using TensorFlow)\n",
        "### Prepared by [Mustafa Youldash, Ph.D.](https://github.com/youldash)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Iris Data Set (i.e., Problem Set)\n",
        "\n",
        "The [Iris data set](https://archive.ics.uci.edu/ml/datasets/Iris/) is a popular data set for classification tasks in machine learning. It consists of 150 samples of iris plants, with each sample consisting of four features (sepal length, sepal width, petal length, and petal width) and a target label indicating the species of the iris plant (setosa, versicolor, or virginica)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To solve the assignment using the Iris data set, students would need to preprocess the data, develop and train a Deep Learning model, and evaluate the performance of the model. Preprocessing the data might involve scaling the features and splitting the data into training and validation sets. Developing and training the model could involve selecting an appropriate architecture and optimization algorithm, setting the learning rate, and choosing the number of epochs. Evaluating the performance of the model could involve using metrics such as accuracy, precision, and recall to assess the model's ability to classify the iris plants correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# What version of Python do you currently have?\n",
        "import sys\n",
        "\n",
        "\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Do you have TensorFlow installed on your system?\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpful Functions for Keras and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from util import helper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "Exploratory Data Analysis (EDA) is a process of analyzing and summarizing a data set in order to understand the underlying structure and relationships within the data. EDA is an important step in the data science process, as it allows you to identify patterns, trends, and anomalies in the data that may not be immediately apparent.\n",
        "\n",
        "There are several benefits of performing EDA for Deep Learning:\n",
        "\n",
        "- EDA helps you understand the data: By performing EDA, you can get a better understanding of the data you are working with, including the distribution of the data, the relationships between different features, and any missing or corrupted values.\n",
        "- EDA can identify potential problems: EDA can help you identify potential problems with the data, such as missing values or outliers, which could impact the performance of your Deep Learning model.\n",
        "- EDA can inform model selection: EDA can help you understand the characteristics of the data, which can inform your choice of Deep Learning model. For example, if the data is highly non-linear, you may want to consider using a model that is capable of capturing complex relationships, such as a neural network.\n",
        "- EDA can improve model performance: By understanding the underlying structure of the data, you can better tune the hyperparameters of your Deep Learning model, which can lead to improved performance.\n",
        "\n",
        "Overall, EDA is an important step in the Deep Learning process, as it helps you understand the data and identify potential issues that could impact the performance of your model. EDA is open-ended, and it is up to you to decide how to look at different ways to slice and dice your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3Df9QwlzgF4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras import models, layers, optimizers, utils\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "LejC3hcD6_5G",
        "outputId": "83c0bff0-6130-4d8d-e09b-debd3b0f320e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#from pandas_profiling.profile_report import ProfileReport\n",
        "\n",
        "iris = pd.read_csv('IRIS.csv', na_values=['NA','?'])\n",
        "iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hint: use a DataFrame for both EDA and model development.\n",
        "iris.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zySYgniaKtCZ",
        "outputId": "bcbba97d-973a-4613-bccb-5f8e0ed5a44f"
      },
      "outputs": [],
      "source": [
        "# Hint: use a DataFrame for both EDA and model development.\n",
        "iris.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris.groupby(\"species\").size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris.species.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDO7ly_pLw8E"
      },
      "outputs": [],
      "source": [
        "iris.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "scatter_matrix(iris)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr = iris.values\n",
        "\n",
        "features= arr[:,:-1]\n",
        "print(features.shape)\n",
        "\n",
        "labels=arr[:,-1]\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "labels=encoder.fit_transform(labels)\n",
        "\n",
        "print(set(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "newlabels=pd.get_dummies(labels).values\n",
        "newlabels[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=np.asarray(features).astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I just split our dataset into 70% for training and 30% for testing after preprocessing and Exploratory Data Analysis (EDA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, valid_data, train_labels, valid_labels= train_test_split(data, newlabels, test_size=0.30, random_state=42)\n",
        "\n",
        "#Scale the features\n",
        "scaler = StandardScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "valid_data = scaler.transform(valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Train Data Shape: \", train_data.shape)\n",
        "print(\"Train Labels Shape: \", train_labels.shape)\n",
        "print(\"Validation Data Shape: \", valid_data.shape)\n",
        "print(\"Validation Labels Shape: \", valid_labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sequential Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I built a Keras sequential model with 4 layers. Firstly, in the input layer, we have 16 neurons, four neurons per feature, and the rest are hidden neurons. Secondly, in the hidden layers, we have two; one consists of 16 neurons, and the other has 8 neurons. Finally, the output layer has three neurons, each one for the classes we have.\n",
        "\n",
        "\n",
        "I compile the model utilizing **Adam** Optimizer, **cotegoral crossentropy** for loss function, and **accuracy** metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define, and build your model.\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model=models.Sequential()\n",
        "model.add(layers.Dense(16, activation=\"relu\", input_shape=(4,), kernel_initializer='normal')) # input layer \n",
        "model.add(layers.Dense(16, activation=\"relu\", kernel_initializer='normal')) # 1st hidden layer \n",
        "model.add(layers.Dense(8, activation=\"relu\", kernel_initializer='normal')) # 2nd hidden layer \n",
        "model.add(layers.Dense(3, activation=\"softmax\")) # output layer \n",
        "\n",
        "# Compile the model.\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I train the model in 60 epochs, with 10 as the batch size, and validation of the model is done by a valid set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history=model.fit(train_data,train_labels,validation_data=(valid_data, valid_labels), epochs=60, batch_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we complete the training, we can now  evaluate  our model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, acc= model.evaluate(valid_data,valid_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Test Accuracy: {}%\".format(acc*100))\n",
        "print(f'Test loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Evaluate the success rate using accuracy.\n",
        "prediction = model.predict(valid_data)\n",
        "\n",
        "y_test_class = np.argmax(valid_labels, axis=1)\n",
        "y_pred_class = np.argmax(prediction, axis=1)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\\n\", metrics.confusion_matrix(y_test_class,y_pred_class))\n",
        "print(\"\\nResult Report:\\n\", metrics.classification_report(y_test_class,y_pred_class))\n",
        "\n",
        "\n",
        "test_scores = model.evaluate(valid_data, valid_labels, verbose=2)\n",
        "print(\"\\nTest loss:\\n\", test_scores[0])\n",
        "print(\"\\nTest accuracy:\\n\", test_scores[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reflection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMIaVyuCQAuUKllqwdbRxBb",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
